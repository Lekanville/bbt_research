#! usr/bin/env python3

#############################################################################################
#The “dat_clean.py” script
#The script expects a folder that contains the temperature recordings (allrecordings.csv) and 
#will output the combined dataset as “data_cleaned.csv”. The script initially combines all 
#individual temperature recording datasets, drops the full duplicates, and creates a primary 
#key by combining the user ID with the start time.
#The primary key created makes it possible to detect and drop partial duplicates. This is done
#by summing all null values across the columns of each row, sorting according to the number of
#the null values in ascending order (The values with zero null values will be at the top), 
#dropping duplicates using the primary key (generated by combining the user ID and the start
#time) while keeping the first row (the row with the least number of null columns, 
#correspondingly, the most complete row).
#Lastly, improperly formatted data columns are cleaned (some data columns are formatted by 
#starting with “Bin”, these seem to cause errors) by sub-setting the portions of the needed 
#string.
#############################################################################################

import numpy as np
import pandas as pd
import os, sys
from loguru import logger
import glob
import argparse
pd.options.mode.chained_assignment = None 


parser = argparse.ArgumentParser(description='A script for initial data cleaning')
parser.add_argument('-i','--input_folder', type=str, required=True, help='The input dataset')
parser.add_argument('-o','--output_file', type=str, required=True, help='The output file')


def remove_bin(df):
   for i in range(len(df)):
    if df.loc[i, "Data"].startswith("Bin"):
        df.loc[i, "Data"] = df.loc[i, "Data"][10:522]
   return df

   
def dat_clean(INPUT_FOLDER, OUTPUT_FILE):
    FILES_PATHS = os.path.join(INPUT_FOLDER, "alluserrecordings*")
    ALL_FILES = glob.glob(FILES_PATHS)

    rec_full  = pd.DataFrame()
    logger.info("loading the datasets...\n")

    for file in ALL_FILES:
        rec = pd.read_csv(file, parse_dates=['Start Time'])
        rec_full = pd.concat([rec_full, rec], axis = 0)
        dataset_name = file.split("/")[-1]
        logger.info(dataset_name, "loaded \n")

    logger.info("Datasets merged \n")

    rec_full_unique = rec_full.drop_duplicates()
    logger.info("Full duplicates dropped \n")

    rec_clean = rec_full_unique.dropna(subset=['Data'])
    logger.info("Null values dropped \n")

    rec_clean["prime"] = (rec_clean["User ID"]).astype(str)+"_"+(rec_clean["Start Time"]).astype(str)
    logger.info("Primary key created \n")

    rec_clean['count'] = rec_clean.isnull().sum(1)
    rec_clean_t = rec_clean.sort_values(['count']).drop_duplicates(subset=['prime'],keep='first').drop(columns = 'count')
    logger.info("Partial duplicates dropped \n")
    
    rec_clean_t.reset_index(drop=True, inplace=True)
    logger.info("Index reset \n")
    
    cleaned = remove_bin(rec_clean_t)
    logger.info("Improperly formated data removed \n")
    
    #cleaned.to_csv(os.path.join(OUTPUT_FOLDER, "data_cleaned.csv"))
    cleaned.to_csv(OUTPUT_FILE)
    logger.info("Data succesfully cleaned and saved")

if __name__ == "__main__":
    args = parser.parse_args()
    dat_clean(args.input_folder, args.output_file)